{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "#### Jakub JÃ³zefowicz\n",
    "#### JJ395253\n",
    "\n",
    "## Implementation overview\n",
    "\n",
    "### Context\n",
    "\n",
    "- Everything was done on the eden cluster on dgx-a100 nodes https://hpc.mini.pw.edu.pl/description/\n",
    "  - This means I didn't have access to Krzysztof Ciebera's c4 dataset, so I just streamed if off huggingface. This can occasionally give a 504, I handed it in using the pl-grid datasets but that doesn't work set `is_plgrid` to false in `scripts/`generate_scripts.ipynb` and run the notebook to regenerate the scripts, then copy them over into project root. Same for account values\n",
    "  - The interconnect between nodes is infiniband, but implements IBoIP, if that isn't the cse on PL-grid then the cpu backend will have to be changed from gloo to mpi.\n",
    "  - The interface used might also have to be specified for dist. comms on some clusters\n",
    "- I didn't bother optimisng NCCL env. variables, but I included them commented out in the scripts\n",
    "- `OMP_NUM_WORKERS` could also be increased maybe\n",
    "- Testing locally the script does seem to use all cores available\n",
    "\n",
    "### ArgParse\n",
    "\n",
    "- Argument Parser was done in the ordinary fashion; I added some bounds checking for negative learning rates etc.\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "- Monitoring was added as Neptune runs with everything in the base namespace including the model params\n",
    "- If you would like the scripts to include monitoring during grading then an api key has to be provided, the scripts can be regenerated with `generate_scripts.ipynb`\n",
    "\n",
    "### Torchrun\n",
    "\n",
    "- one task per node, srun is using in combination with torchrun\n",
    "- The master address was set according to a comment someone made in this gist: https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904\n",
    "  - hostname seems to work, but if not then one could srun one node to get it's ip\n",
    "- The master port is randomly generated and is $\\geq 10000$\n",
    "- The c10d backend is used, with `--standalone` used for the basic torchrun\n",
    "\n",
    "### FSDP\n",
    "\n",
    "- Used `transformer_auto_wrap_policy` on the `Block` module provided\n",
    "- Mixed precision is set to `bf16` in the manner requested if it is available on the node, this info is logged\n",
    "\n",
    "### 2 GPUs\n",
    "\n",
    "- No substantial changes required for the training scripts. Just change num nodes in batch and torchrun adjusting the number of gpus per node\n",
    "- The dataloading was done with `split_dataset_by_node` provided by huggingface; `DistributedSampler` doesn't work with `IterableDataset`\n",
    "\n",
    "### Cosine Learning\n",
    "\n",
    "- Just used a `SequentialLR` for `(LinearLR, CosineAnnealingLR)`\n",
    "  - `T_max` was set to `0.99 * train_steps`\n",
    "  - No warm restarts were used, since they didn't seem to be used in the optimal compute paper\n",
    "\n",
    "### Optimal Training steps\n",
    "\n",
    "- Calculated below\n",
    "- I just used `D=20N`, I don't see anything more in the paper that has to be extrapolated\n",
    "- This was the only part I ran on two nodes\n",
    "- Used `#SBATCH --array=2-3`\n",
    "\n",
    "### Saving and loading the model\n",
    "\n",
    "- Ran it with dependecies\n",
    "```\n",
    "jjozefowicz@eden:~/git/bml-big-2$ sbatch sbatch_save.sub\n",
    "Submitted batch job 977273\n",
    "jjozefowicz@eden:~/git/bml-big-2$ sbatch -d afterok:977273 sbatch_load.sub\n",
    "Submitted batch job 977274\n",
    "```\n",
    "- I used `from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict` instead of the FSDP equivalent since the latter is deprecated and it was suggested to use the former instead.\n",
    "- I save:\n",
    "  - Model params\n",
    "  - Optimizer state\n",
    "  - Scheduler state\n",
    "  - No. of steps ran\n",
    "    - It seems like for huggingface datasets you have to just skip the first $n$ samples instead of setting some dataloader state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating number of train steps\n",
    "We just calculate the number of parameters manually and then multiply by 20 from what I understand. I don't see what else we need from the publication if we are given $D = 20N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 28950016, D:579000320\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "d_model = embed_dim = 256\n",
    "sequence_length = max_length = 256\n",
    "num_layers = 4\n",
    "num_heads = 4\n",
    "# embedding\n",
    "embeds = (vocab_size * embed_dim) + (sequence_length * embed_dim)\n",
    "# transformer blocks\n",
    "## fully connected\n",
    "layer_norm = 2 * d_model\n",
    "ff_in = d_model * (4 * d_model) + (4 * d_model)  # bias\n",
    "ff_out = (4 * d_model) * d_model + d_model  # bias\n",
    "ff = layer_norm + ff_in + ff_out\n",
    "## self-attention\n",
    "attn_in_proj = d_model * (3 * d_model)  # no bias\n",
    "attn_out_proj = d_model * d_model  # no bias\n",
    "attn = (\n",
    "    attn_in_proj + attn_out_proj\n",
    ")  # it seems like d_model of a head is d_model/num_heads\n",
    "blocks = num_layers * (ff + attn)\n",
    "# final head\n",
    "head = d_model * vocab_size\n",
    "# In total we get\n",
    "N = embeds + blocks + head\n",
    "D = 20 * N\n",
    "print(f\"N: {N}, D:{D}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now for number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 8835\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "per_step = batch_size * sequence_length\n",
    "\n",
    "num_steps = round(D / per_step)\n",
    "print(f\"Num steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Cosine scheduler\n",
    "\n",
    "![](assets/cosine.png)\n",
    "\n",
    "### Multi GPU\n",
    "\n",
    "![](assets/multi-gpu.png)\n",
    "\n",
    "### Learning Rate Comparison\n",
    "\n",
    "![](assets/lr.png)\n",
    "\n",
    "### Checkpoint Save-Load\n",
    "\n",
    "![](assets/save_load.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

#!/bin/bash
#SBATCH --job-name=save-submit
#SBATCH --cpus-per-gpu=8
#SBATCH --gpus-per-task=2
#SBATCH --mem-per-gpu=64G
#SBATCH --ntasks-per-node=1
#SBATCH --nodes=1

#SBATCH --output=R-%x.%j.out
#SBATCH --error=R-%x.%j.err

#SBATCH --partition=plgrid-gpu-a100
#SBATCH --account=plgllmparamgr-gpu-a100
#SBATCH --time=00:10:00

#export OMP_NUM_THREADS=2
#export NCCL_NSOCKS_PERTHREAD=4
#export NCCL_SOCKET_NTHREADS=2
#export NCCL_MIN_NCHANNELS=32



# https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904
echo "NODELIST="${SLURM_NODELIST}
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
echo "MASTER_ADDR="$MASTER_ADDR
export MASTER_PORT=$((10000 + $RANDOM))
echo "MASTER_PORT="$MASTER_PORT
id=$RANDOM

srun torchrun \
--nnodes=1 \
--nproc_per_node=2 \
--rdzv_id $id \
--rdzv_backend c10d \
--rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
main.py --is_plgrid -n 100  --save_dir saved-model \
--log_valid_loss_freq 200 --log_train_loss_freq 5 --early_stop 50


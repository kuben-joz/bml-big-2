#!/bin/bash
#SBATCH --array=2-3
#SBATCH --job-name=grid-search-submit-pascal
#SBATCH --cpus-per-gpu=8
#SBATCH --gpus-per-task=2
#SBATCH --mem-per-gpu=64G
#SBATCH --ntasks-per-node=1
#SBATCH --nodes=1

#SBATCH --output=R-%x.%j.out
#SBATCH --error=R-%x.%j.err

#SBATCH --partition=experimental
#SBATCH --account=sfglab
#SBATCH --time=00:10:00

#export OMP_NUM_THREADS=2
#export NCCL_NSOCKS_PERTHREAD=4
#export NCCL_SOCKET_NTHREADS=2
#export NCCL_MIN_NCHANNELS=32

source data/export.env

# https://gist.github.com/TengdaHan/1dd10d335c7ca6f13810fff41e809904
echo "NODELIST="${SLURM_NODELIST}
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
echo "MASTER_ADDR="$MASTER_ADDR
export MASTER_PORT=$((10000 + $RANDOM))
echo "MASTER_PORT="$MASTER_PORT
id=$RANDOM

srun torchrun \
--nnodes=1 \
--nproc_per_node=2 \
--rdzv_id $id \
--rdzv_backend c10d \
--rdzv_endpoint $MASTER_ADDR:$MASTER_PORT \
main.py --no-is_plgrid -n todo --log \
--num_layers 4 --num_heads 4 --d_model 256 --seq_length 256 -bs 4 --dropout 0.0 -lr 1e-$SLURM_ARRAY_TASK_ID
# batch size is per gpu so 128